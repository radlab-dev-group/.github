<section class="hero">
    <div class="container">
        <h1 data-i18n="hero.title">Open-Source AI Gateway for Local and Cloud LLM Infrastructure</h1>
        <p data-i18n="hero.description" data-i18n-html="true">
            LLM Router is a flexible, open-source gateway for managing both local LLM deployments (vLLM, Ollama, LM
            Studio) and cloud providers. It provides an intelligent layer between your applications and models,
            delivering real-time traffic management, intelligent load balancing across local and cloud providers, and
            comprehensive security controls including PII masking, data anonymization, and content filtering. Built with
            <strong>Apache 2.0</strong> license, it's designed for teams who want to run AI infrastructure locally with
            optional cloud provider supportâ€”giving you full control and flexibility.
        </p>

        <div class="cta-buttons">
            <a href="https://github.com/radlab-dev-group/llm-router"
               class="btn btn-primary"
               target="_blank" data-i18n="hero.cta.github">View on GitHub
            </a>
            <a href="https://github.com/radlab-dev-group/llm-router/blob/main/README.md"
               class="btn btn-secondary"
               target="_blank" data-i18n="hero.cta.docs">
                Documentation
            </a>
        </div>
    </div>
</section>