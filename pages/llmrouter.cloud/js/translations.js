export const translations = {
  pl: {
    // Header
    'header.menu.aria': 'OtwÃ³rz menu',
    'header.nav.description': 'FunkcjonalnoÅ›ci',
    'header.nav.security': 'BezpieczeÅ„stwo',
    'header.nav.performance': 'WydajnoÅ›Ä‡',
    'header.nav.useCases': 'Zastosowania',
    'header.nav.contact': 'Kontakt',
    'header.github.router.tooltip': 'gÅ‚Ã³wne repozytorium llm-router.cloud na GitHub',
    'header.github.page.tooltip': 'strona llmrouter na GitHub',

    // Hero
    'hero.title': 'Brama AI dla lokalnej i chmurowej infrastruktury LLM',
    'hero.subtitle.reliable': 'Niezawodny',
    'hero.subtitle.text': 'routing dla duÅ¼ych modeli jÄ™zykowych z wbudowanym',
    'hero.subtitle.security': 'zabezpieczeniem danych',
    'hero.description': 'Elastyczna brama open-source do zarzÄ…dzania lokalnymi wdroÅ¼eniami LLM (vLLM, Ollama, LM Studio) oraz dostawcami chmurowymi. Zapewnia inteligentnÄ… warstwÄ™ miÄ™dzy aplikacjami a modelami, oferujÄ…c zarzÄ…dzanie ruchem w czasie rzeczywistym. Inteligentne rÃ³wnowaÅ¼enie obciÄ…Å¼enia miÄ™dzy lokalnymi i chmurowymi dostawcami oraz kompleksowe mechanizmy bezpieczeÅ„stwa, w tym maskowanie danych osobowych, anonimizacja i filtrowanie treÅ›ci. Zbudowana na licencji <strong>Apache 2.0</strong>, zostaÅ‚a zaprojektowana dla zespoÅ‚Ã³w, ktÃ³re chcÄ… uruchamiaÄ‡ infrastrukturÄ™ AI lokalnie z opcjonalnym wsparciem dostawcÃ³w chmurowychâ€”majÄ…c peÅ‚nÄ… kontrolÄ™ i elastycznoÅ›Ä‡.',
    'hero.cta.github': 'Zobacz na GitHub',
    'hero.cta.docs': 'Dokumentacja',

    // Features
    'features.title': 'Kompleksowe zarzÄ…dzanie infrastrukturÄ… LLM',
    'features.subtitle': '<strong>Kompletna</strong> platforma do zarzÄ…dzania <strong>lokalnymi wdroÅ¼eniami LLM (vLLM, Ollama, LM Studio) i opcjonalnie dostawcami chmurowymi</strong> w dowolnej skali. Zunifikowane zarzÄ…dzanie z produkcyjnym bezpieczeÅ„stwem i obserwowalnoÅ›ciÄ…. Architektura open-source umoÅ¼liwia dostosowywanie i rozwÃ³j wspierany przez spoÅ‚ecznoÅ›Ä‡. <strong>Uruchamiaj modele lokalnie</strong> z peÅ‚nÄ… kontrolÄ… nad infrastrukturÄ….',
    'features.anonymization.title': 'Wbudowana ochrona danych',
    'features.anonymization.desc': 'NiezaleÅ¼nie konfigurowalne pipeliny chroniÄ… dane przed wysÅ‚aniem do dostawcÃ³w modeli. Metody typu guardrails w pipeline blokujÄ… ruch do dostawcy w momencie wykrycia treÅ›ci niezgodnych z zasadami etyki. Natomiast metody maskujÄ…ce w pipeline majÄ… za zadanie ukryÄ‡ chronione informacje, zanim zostanÄ… one wysÅ‚ane na zewnÄ…trz.',
    'features.loadbalancing.title': 'Elastyczne rÃ³wnowaÅ¼enie obciÄ…Å¼enia',
    'features.loadbalancing.desc': 'Cztery konfigurowalne strategie rÃ³wnowaÅ¼enia obciÄ…Å¼enia: <code>balanced</code>, <code>weighted</code>, <code>first_available</code>, <code>first_available_optim</code>. Optymalizacja wykorzystania zasobÃ³w i zapewnienie wysokiej dostÄ™pnoÅ›ci dla wdroÅ¼eÅ„ dowolnej wielkoÅ›ci.',
    'features.routing.title': 'Zunifikowany routing lokalny i chmurowy',
    'features.routing.desc': 'Pojedyncze REST API obsÅ‚ugujÄ…ce lokalnych dostawcÃ³w (Ollama, vLLM, LM Studio) oraz usÅ‚ugi chmurowe (OpenAI, Anthropic, itd.). Automatyczna konwersja protokoÅ‚Ã³w, pÅ‚ynne przeÅ‚Ä…czanie miÄ™dzy lokalnymi i chmurowymi dostawcami oraz inteligentna dystrybucja obciÄ…Å¼enia na wszystkich endpointach.',
    'features.monitoring.title': 'PeÅ‚na obserwowalnoÅ›Ä‡',
    'features.monitoring.desc': 'Natywna integracja z Prometheus dostarczajÄ…ca metryki w czasie rzeczywistym: wspÃ³Å‚czynniki Å¼Ä…daÅ„, percentyle opÃ³ÅºnieÅ„, wspÃ³Å‚czynniki bÅ‚Ä™dÃ³w na dostawcÄ™ oraz health checks. PeÅ‚na widocznoÅ›Ä‡ przez panele Grafana z automatycznym wykrywaniem usÅ‚ug i alertami.',
    'features.performance.title': 'Wysoka wydajnoÅ›Ä‡',
    'features.performance.desc': 'Strumieniowanie odpowiedzi, pooling poÅ‚Ä…czeÅ„, broker Redis do wdroÅ¼eÅ„ rozproszonych. Zoptymalizowane pod kÄ…tem minimalnego opÃ³Åºnienia i maksymalnej przepustowoÅ›ci w rÃ³Å¼nych scenariuszach wdroÅ¼eÅ„.',
    'features.config.title': 'Åatwe wdroÅ¼enie',
    'features.config.desc': 'Konfiguracja infrastruktury jako kodu przez JSON i zmienne Å›rodowiskowe. GotowoÅ›Ä‡ na Docker i Kubernetes z oficjalnymi obrazami. WdroÅ¼enie w minuty z elastycznymi opcjami konfiguracji dla dowolnego Å›rodowiska.',
    'features.endpoints.title': 'Rozszerzalne endpointy',
    'features.endpoints.desc': 'Gotowe do uÅ¼ycia endpointy dla pipeline\'Ã³w RAG, generowania pytaÅ„, tÅ‚umaczeÅ„ i innych typowych workflow AI. Åatwo rozszerzalne o niestandardowe przypadki uÅ¼ycia.',
    'features.web.title': 'Interfejsy zarzÄ…dzania',
    'features.web.desc': 'Configuration Manager do centralnej kontroli infrastruktury i interfejs Anonymizer do bezpiecznej, zgodnej komunikacji z LLM.',

    // Security
    'security.title': 'Architektura z bezpieczeÅ„stwem na pierwszym miejscu',
    'security.description': 'LLM Router zostaÅ‚ zaprojektowany z myÅ›lÄ… o Å›rodowiskach gdzie bezpieczeÅ„stwo danych oraz zgodnoÅ›Ä‡ z SOC2 ma znaczenie. KaÅ¼de Å¼Ä…danie jest automatycznie chronione przez dwuwarstwowy system bezpieczeÅ„stwa: guardrails walidujÄ…ce bezpieczeÅ„stwo treÅ›ci przed przetwarzaniem oraz pipeline maskerÃ³w anonimizujÄ…ce wraÅ¼liwe dane przed dalszÄ… transmisjÄ….',
    'security.guardrails.title': 'Guardrails - Warstwa BezpieczeÅ„stwa TreÅ›ci',
    'security.guardrails.feature1': 'Wieloetapowe pipeline guardrails do walidacji i kontroli bezpieczeÅ„stwa treÅ›ci',
    'security.guardrails.feature2': 'Integracja NASK Guard dla bezpieczeÅ„stwa treÅ›ci w jÄ™zyku polskim (model NASK-PIB/PL-Guard)',
    'security.guardrails.feature3': 'Rozszerzalna architektura pluginÃ³w dla wÅ‚asnych implementacji guardrails',
    'security.guardrails.feature4': 'Blokowanie Å¼Ä…daÅ„ z niebezpiecznÄ… lub zabronionÄ… treÅ›ciÄ… przed przetwarzaniem LLM',
    'security.guardrails.feature5': 'Konfigurowalne Å‚aÅ„cuchy guardrails z sekwencyjnymi etapami walidacji',
    'security.masking.title': 'Maskowanie Danych - Warstwa Ochrony PII',
    'security.masking.feature1': 'Automatyczne wykrywanie i maskowanie danych osobowych, finansowych, email',
    'security.masking.feature2': 'Wsparcie dla miÄ™dzynarodowych identyfikatorÃ³w: SSN, NIP, REGON, numery ID,',
    'security.masking.feature4': 'Walidacja sum kontrolnych wykrytych identyfikatorÃ³w zapewniajÄ…ca dokÅ‚adnoÅ›Ä‡',
    'security.masking.feature5': 'Rozszerzalny silnik maskowania z obsÅ‚ugÄ… reguÅ‚ niestandardowych dla wymagaÅ„ branÅ¼owych',
    'security.masking.feature6': 'Pipeline wielu pluginÃ³w maskujÄ…cych z sekwencyjnÄ… transformacjÄ… danych',
    'security.masking.feature7': 'Wymuszona anonimizacja z konfigurowalnymi strategiami dla caÅ‚ego payloadu',
    'security.code.comment1': '# =======================================',
    'security.code.comment2': '# PrzykÅ‚ad: Automatyczna anonimizacja',
    'security.code.comment3': '# Przed transmisjÄ… do LLM:',
    'security.code.comment4': '# Po anonimizacji:',

    // Performance
    'performance.title': 'Skalowalna optymalizacja zasobÃ³w',
    'performance.subtitle': 'Zaawansowane mechanizmy orkiestracji zapewniajÄ… maksymalnÄ… efektywnoÅ›Ä‡ i optymalne wykorzystanie zasobÃ³w, inteligentnie dystrybuujÄ…c obciÄ…Å¼enia miÄ™dzy dostawcÃ³w przy uÅ¼yciu konfigurowalnych strategii rÃ³wnowaÅ¼enia obciÄ…Å¼enia. Skaluje siÄ™ od wdroÅ¼eÅ„ na pojedynczym serwerze do rozproszonej infrastruktury produkcyjnej.',
    'performance.stat1.number': '4',
    'performance.stat1.label': 'Strategie rÃ³wnowaÅ¼enia obciÄ…Å¼enia',
    'performance.stat2.number': 'âˆ',
    'performance.stat2.label': 'ObsÅ‚ugiwanych dostawcÃ³w LLM',
    'performance.stat3.number': 'âœ“',
    'performance.stat3.label': 'Pipeline Guardrails',
    'performance.stat4.number': 'âœ“',
    'performance.stat4.label': 'Pipeline MaskerÃ³w',
    'performance.stat5.number': '15+',
    'performance.stat5.label': 'ReguÅ‚ maskowania danych osobowych',
    'performance.stat6.number': '100%',
    'performance.stat6.label': 'Open Source Apache 2.0',

    // Use Cases
    'usecases.title': 'Przypadki uÅ¼ycia',
    'usecases.subtitle': 'LLM Router dostarcza wartoÅ›Ä‡ w rÃ³Å¼norodnych scenariuszach dla zespoÅ‚Ã³w deweloperskich i operacyjnych. NiezaleÅ¼nie od tego, czy budujesz nowe aplikacje oparte na AI, modernizujesz istniejÄ…ce systemy czy eksperymentujesz z modelami, zapewnia elastycznÄ… infrastrukturÄ™ skalujÄ…cÄ… siÄ™ z Twoimi potrzebami.',
    'usecases.corporate.title': 'ğŸ¢ Scentralizowane zarzÄ…dzanie AI',
    'usecases.corporate.problem': '<strong>Wyzwanie:</strong> Rozproszony dostÄ™p do LLM w zespoÅ‚ach bez ujednoliconego zarzÄ…dzania, kontroli bezpieczeÅ„stwa czy widocznoÅ›ci kosztÃ³w.',
    'usecases.corporate.solution': '<strong>RozwiÄ…zanie:</strong> LLM Router centralizuje caÅ‚y ruch AI przez pojedynczÄ… bramÄ™, umoÅ¼liwiajÄ…c kompleksowe zarzÄ…dzanie danymi, automatycznÄ… zgodnoÅ›Ä‡ przez anonimizacjÄ™ danych osobowych oraz scentralizowanÄ… kontrolÄ™ dostÄ™pu zanim Å¼Ä…dania dotrÄ… do dostawcy.',
    'usecases.corporate.feature1': 'Scentralizowana kontrola dostÄ™pu i polityk',
    'usecases.corporate.feature2': 'PeÅ‚ny Å›lad audytu dla zgodnoÅ›ci',
    'usecases.corporate.feature3': 'Brak narzutu rate limitingu, zoptymalizowana przepustowoÅ›Ä‡',
    'usecases.highavailability.title': 'ğŸ“ˆ Systemy wysokiej dostÄ™pnoÅ›ci',
    'usecases.highavailability.problem': '<strong>Wyzwanie:</strong> Awarie dostawcÃ³w powodujÄ… caÅ‚kowity przestÃ³j aplikacji, zakÅ‚Ã³cajÄ…c operacje.',
    'usecases.highavailability.solution': '<strong>RozwiÄ…zanie:</strong> Automatyczne przeÅ‚Ä…czanie awaryjne miÄ™dzy dostawcami z ciÄ…gÅ‚ym monitorowaniem stanu i circuit breakerami. OsiÄ…gnij wysokÄ… dostÄ™pnoÅ›Ä‡ przez inteligentne rÃ³wnowaÅ¼enie obciÄ…Å¼enia, redundancjÄ™ dostawcÃ³w i routing ruchu w czasie rzeczywistym z opcjonalnÄ… anonimizacjÄ… w locie.',
    'usecases.highavailability.feature1': 'Dystrybucja obciÄ…Å¼enia miÄ™dzy wieloma dostawcami',
    'usecases.highavailability.feature2': 'Health checks w czasie rzeczywistym na dostawcÄ™',
    'usecases.highavailability.feature3': 'Automatyczne przeÅ‚Ä…czanie awaryjne i circuit breaking',
    'usecases.security.title': 'ğŸ›¡ï¸ Wielowarstwowe BezpieczeÅ„stwo',
    'usecases.security.problem': '<strong>Wyzwanie:</strong> Walidacja bezpieczeÅ„stwa treÅ›ci, jak i ochrony danych przed wysÅ‚aniem Å¼Ä…daÅ„ do dostawcÃ³w LLM.',
    'usecases.security.solution': '<strong>RozwiÄ…zanie:</strong> LLM Router zapewnia dwuwarstwowy system bezpieczeÅ„stwa: pipeline guardrails walidujÄ… bezpieczeÅ„stwo treÅ›ci i blokujÄ… zabronione Å¼Ä…dania, podczas gdy pipeline maskerÃ³w automatycznie anonimizujÄ… wraÅ¼liwe dane. Konfiguruj sekwencyjne etapy przetwarzania zarÃ³wno dla guardrails jak i maskerÃ³w, zapewniajÄ…c peÅ‚nÄ… ochronÄ™ zanim Å¼Ä…danie dotrze do dostawcy LLM.',
    'usecases.security.feature1': 'Sekwencyjna walidacja guardrails',
    'usecases.security.feature2': 'Maskowanie PII z 15+ typami reguÅ‚',
    'usecases.security.feature3': 'Architektura pipeline dla kompozycyjnych warstw bezpieczeÅ„stwa',
    'usecases.local.title': 'ğŸ–¥ï¸ WÅ‚asne lokalne modele',
    'usecases.local.problem': '<strong>Wyzwanie:</strong> Uruchamianie modeli na lokalnej infrastrukturze (vLLM, Ollama, LM Studio) wymaga rÄ™cznego zarzÄ…dzania wieloma endpointami i protokoÅ‚ami.',
    'usecases.local.solution': '<strong>RozwiÄ…zanie:</strong> LLM Router zapewnia zunifikowane zarzÄ…dzanie wszystkimi lokalnymi wdroÅ¼eniami modeli. Uruchamiaj modele na wÅ‚asnym sprzÄ™cie z peÅ‚nÄ… kontrolÄ…, dystrybuuj obciÄ…Å¼enie miÄ™dzy wiele lokalnych instancji i opcjonalnie dodaj dostawcÃ³w chmurowych jako backup. Zachowaj dane i przetwarzanie caÅ‚kowicie w swojej infrastrukturze.',
    'usecases.local.feature1': 'PeÅ‚na suwerennoÅ›Ä‡ i kontrola nad danymi',
    'usecases.local.feature2': 'RÃ³wnowaÅ¼enie obciÄ…Å¼enia miÄ™dzy lokalnymi instancjami',
    'usecases.local.feature3': 'Opcjonalny fallback do chmury przy szczytowym obciÄ…Å¼eniu',
    'usecases.research.title': 'ğŸ”¬ Eksperymentowanie i testy A/B',
    'usecases.research.problem': '<strong>Wyzwanie:</strong> Testowanie rÃ³Å¼nych modeli i dostawcÃ³w wymaga zmian kodu i skomplikowanej konfiguracji infrastruktury.',
    'usecases.research.solution': '<strong>RozwiÄ…zanie:</strong> Eksperymentuj z lokalnymi modelami (vLLM, Ollama, LM Studio) i dostawcami chmurowymi bez Å¼adnych zmian w kodzie aplikacji. Routing sterowany konfiguracjÄ… umoÅ¼liwia pÅ‚ynne przeÅ‚Ä…czanie, testy A/B miÄ™dzy rÃ³Å¼nymi modelami i strategie zoptymalizowane pod kÄ…tem kosztÃ³w priorytetyzujÄ…ce lokalne wdroÅ¼enia.',
    'usecases.research.feature1': 'PorÃ³wnywanie wydajnoÅ›ci modeli lokalnych vs. chmurowych',
    'usecases.research.feature2': 'Testy A/B dowolnej kombinacji modeli',
    'usecases.research.feature3': 'Optymalizacja kosztÃ³w z routingiem lokalnym-najpierw',
    'usecases.realtime.title': 'ğŸš€ Przetwarzanie w czasie rzeczywistym z niskim opÃ³Åºnieniem',
    'usecases.realtime.problem': '<strong>Wyzwanie:</strong> OpÃ³Åºnienia przetwarzania i brak obsÅ‚ugi strumieniowania wpÅ‚ywajÄ…ce na doÅ›wiadczenie uÅ¼ytkownika w aplikacjach czasu rzeczywistego.',
    'usecases.realtime.solution': '<strong>RozwiÄ…zanie:</strong> LLM Router obsÅ‚uguje Server-Sent Events (SSE) i strumieniowanie odpowiedzi z automatycznÄ… konwersjÄ… protokoÅ‚Ã³w. Zoptymalizowany pod kÄ…tem minimalnego opÃ³Åºnienia z poolingiem poÅ‚Ä…czeÅ„, centralizacjÄ… Redis dla systemÃ³w rozproszonych i nieblokujÄ…cÄ… obsÅ‚ugÄ… Å¼Ä…daÅ„.',
    'usecases.realtime.feature1': 'Strumieniowanie SSE <span class="arrow-icon">&#8596;</span> konwersja Ollama',
    'usecases.realtime.feature2': 'Pooling poÅ‚Ä…czeÅ„ i pipelining',
    'usecases.realtime.feature3': 'Rozproszona koordynacja Redis',
    'usecases.legacy.title': 'âš¡ Modernizacja systemÃ³w legacy',
    'usecases.legacy.problem': '<strong>Wyzwanie:</strong> Aplikacje legacy nie majÄ… nowoczesnych interfejsÃ³w do integracji LLM, wymagajÄ…c obszernego refaktoringu.',
    'usecases.legacy.solution': '<strong>RozwiÄ…zanie:</strong> WdrÃ³Å¼ LLM Router jako ujednoliconÄ… warstwÄ™ proxy zapewniajÄ…cÄ… spÃ³jny dostÄ™p REST API z dowolnej aplikacjiâ€”niezaleÅ¼nie od wersji, architektury czy jÄ™zyka programowania. Modernizuj moÅ¼liwoÅ›ci AI bez refaktoringu istniejÄ…cych baz kodu.',
    'usecases.legacy.feature1': 'Ujednolicona brama REST API',
    'usecases.legacy.feature2': 'Uniwersalna kompatybilnoÅ›Ä‡ z dostawcami',
    'usecases.legacy.feature3': 'Integracja legacy bez zmian',

    // Open Source
    'opensource.title': 'Open Source i rozwÃ³j wspierany przez spoÅ‚ecznoÅ›Ä‡',
    'opensource.subtitle': 'Transparentny, rozszerzalny, gotowy do produkcji',
    'opensource.license': 'Licencja:',
    'opensource.description': 'LLM Router jest <strong>w peÅ‚ni open source</strong> na permisywnej licencji <strong>Apache 2.0</strong>, zapewniajÄ…c peÅ‚nÄ… swobodÄ™ <strong>uÅ¼ytku komercyjnego</strong>, <strong>modyfikacji</strong> i <strong>dystrybucji</strong>. Zbudowany na wspÃ³Å‚pracy spoÅ‚ecznoÅ›ci z transparentnÄ… architekturÄ… i audytowalnym kodem, moÅ¼esz go dostosowaÄ‡ do swoich konkretnych potrzeb, wnieÅ›Ä‡ ulepszenia i korzystaÄ‡ z ciÄ…gÅ‚ych ulepszeÅ„ spoÅ‚ecznoÅ›ciâ€”wszystko bez vendor lock-in czy ograniczeÅ„ licencyjnych.',
    'opensource.cta.star': 'â­ Gwiazdka na GitHub',
    'opensource.cta.issue': 'ğŸ’¡ ZgÅ‚oÅ› problem',
    'opensource.cta.contribute': 'ğŸ”§ WspÃ³Å‚twÃ³rz',

    // Contact
    'contact.title': 'Kontakt',
    'contact.subtitle': 'Masz pytania? Napisz do nas, a my odpowiemy bÅ‚yskawicznie!',

    // Footer
    'footer.project': 'Projekt',
    'footer.project.repo': 'Repozytorium GitHub',
    'footer.project.docs': 'Dokumentacja',
    'footer.project.changelog': 'Changelog',
    'footer.project.license': 'Licencja',
    'footer.components': 'Komponenty',
    'footer.community': 'SpoÅ‚ecznoÅ›Ä‡',
    'footer.community.issues': 'Issue Tracker',
    'footer.community.pulls': 'Pull Requests',
    'footer.community.discussions': 'Dyskusje',
    'footer.community.radlab': 'RadLab Dev Group',
    'footer.resources': 'Zasoby',
    'footer.resources.quickstart': 'Szybki start',
    'footer.resources.install': 'Instalacja',
    'footer.resources.docker': 'Obrazy Docker',
    'footer.resources.mlutils': 'ML Utils (zaleÅ¼noÅ›Ä‡)',
    'footer.copyright': 'Â© 2025 LLM-Router | Projekt Open Source | Licencja Apache 2.0',

    // Back to top
    'backToTop.aria': 'PowrÃ³t na gÃ³rÄ™',
  },

  en: {
    // Header
    'header.menu.aria': 'Open menu',
    'header.nav.description': 'Features',
    'header.nav.security': 'Security',
    'header.nav.performance': 'Performance',
    'header.nav.useCases': 'Use Cases',
    'header.nav.contact': 'Contact',
    'header.github.router.tooltip': 'main llm-router.cloud GitHub repository',
    'header.github.page.tooltip': 'llmrouter website on GitHub',

    // Hero
    'hero.title': 'Open-Source AI Gateway for Local and Cloud LLM Infrastructure',
    'hero.subtitle.reliable': 'Reliable',
    'hero.subtitle.text': 'routing for large language models with built-in',
    'hero.subtitle.security': 'data protection',
    'hero.description': 'LLM Router is a flexible, open-source gateway for managing both local LLM deployments (vLLM, Ollama, LM Studio) and cloud providers. It provides an intelligent layer between your applications and models, delivering real-time traffic management, intelligent load balancing across local and cloud providers, and comprehensive security controls including PII masking, data anonymization, and content filtering. Built with <strong>Apache 2.0</strong> license, it\'s designed for teams who want to run AI infrastructure locally with optional cloud provider supportâ€”giving you full control and flexibility.',
    'hero.cta.github': 'View on GitHub',
    'hero.cta.docs': 'Documentation',

    // Features
    'features.title': 'Comprehensive LLM Infrastructure Management',
    'features.subtitle': '<strong>Complete</strong> platform for managing <strong>local LLM deployments (vLLM, Ollama, LM Studio) and optionally cloud providers</strong> at any scale. Unified management with production-grade security and observability. Open-source architecture enables customization and community-driven enhancements. <strong>Run your models locally</strong> with full control over your infrastructure.',
    'features.anonymization.title': 'Built-in Data Protection',
    'features.anonymization.desc': 'Independently configurable pipelines protect data before sending to model providers. Guardrail methods in the pipeline block traffic to the provider when content violating ethical policies is detected. Masking methods in the pipeline hide protected information before it is sent externally.',
    'features.loadbalancing.title': 'Flexible Load Balancing',
    'features.loadbalancing.desc': 'Four configurable load balancing strategies: <code>balanced</code>, <code>weighted</code>, <code>first_available</code>, <code>first_available_optim</code>. Optimize resource utilization and ensure high availability for any deployment size.',
    'features.routing.title': 'Unified Local & Cloud Routing',
    'features.routing.desc': 'Single REST API supporting local providers (Ollama, vLLM, LM Studio) and cloud services (OpenAI, Anthropic, etc.). Automatic protocol conversion, seamless switching between local and cloud providers, and intelligent load distribution across all endpoints.',
    'features.monitoring.title': 'Full Observability',
    'features.monitoring.desc': 'Native Prometheus integration delivering real-time metrics: request rates, latency percentiles, error rates per provider, and health checks. Complete visibility through Grafana dashboards with automatic service discovery and alerting.',
    'features.performance.title': 'High Performance',
    'features.performance.desc': 'Streaming responses, connection pooling, Redis broker for distributed deployments. Optimized for minimal latency and maximum throughput across various deployment scenarios.',
    'features.config.title': 'Easy Deployment',
    'features.config.desc': 'Infrastructure-as-code configuration via JSON and environment variables. Docker and Kubernetes ready with official images. Deploy in minutes with flexible configuration options for any environment.',
    'features.endpoints.title': 'Extensible Endpoints',
    'features.endpoints.desc': 'Ready-to-use endpoints for RAG pipelines, question generation, translation, and other common AI workflows. Easily extensible for custom use cases.',
    'features.web.title': 'Management Interfaces',
    'features.web.desc': 'Configuration Manager for centralized infrastructure control and Anonymizer interface for secure, compliant LLM communication.',

    // Security
    'security.title': 'Security-First Architecture',
    'security.description': 'LLM Router was designed with environments in mind where data security and SOCâ€¯2 compliance matter. Every request is automatically protected by a twoâ€‘layer security system: guardrails that validate content safety before processing, and a masking pipeline that anonymizes sensitive data before further transmission.',
    'security.guardrails.title': 'Guardrails - Content Safety Layer',
    'security.guardrails.feature1': 'Multi-stage guardrail pipelines for content validation and safety control',
    'security.guardrails.feature2': 'NASK Guard integration for Polish language content safety (NASK-PIB/PL-Guard model)',
    'security.guardrails.feature3': 'Extensible plugin architecture for custom guardrail implementations',
    'security.guardrails.feature4': 'Blocking requests with unsafe or prohibited content before LLM processing',
    'security.guardrails.feature5': 'Configurable guardrail chains with sequential validation stages',
    'security.masking.title': 'Data Masking - PII Protection Layer',
    'security.masking.feature1': 'Automatic detection and masking of personal, financial, and email data',
    'security.masking.feature2': 'Support for international identifiers: SSN, NIP, REGON, ID numbers',
    'security.masking.feature4': 'Checksum validation of detected identifiers ensuring accuracy',
    'security.masking.feature5': 'Extensible masking engine with custom rule support for industry-specific requirements',
    'security.masking.feature6': 'Multi-plugin masking pipeline with sequential data transformation',
    'security.masking.feature7': 'Enforced anonymization with configurable strategies for entire payload',
    'security.code.comment1': '# =======================================',
    'security.code.comment2': '# Example: Automatic Anonymization',
    'security.code.comment3': '# Before transmission to LLM:',
    'security.code.comment4': '# After anonymization:',

    // Performance
    'performance.title': 'Scalable Resource Optimization',
    'performance.subtitle': 'Advanced orchestration mechanisms deliver maximum efficiency and optimal resource utilization, intelligently distributing workloads across providers using configurable load balancing strategies. Scales from single-server deployments to distributed production infrastructure.',
    'performance.stat1.number': '4',
    'performance.stat1.label': 'Load Balancing Strategies',
    'performance.stat2.number': 'âˆ',
    'performance.stat2.label': 'LLM Providers Supported',
    'performance.stat3.number': 'âœ“',
    'performance.stat3.label': 'Guardrail Pipelines',
    'performance.stat4.number': 'âœ“',
    'performance.stat4.label': 'Masking Pipelines',
    'performance.stat5.number': '15+',
    'performance.stat5.label': 'PII Masking Rules',
    'performance.stat6.number': '100%',
    'performance.stat6.label': 'Open Source Apache 2.0',

    // Use Cases
    'usecases.title': 'Use Cases',
    'usecases.subtitle': 'LLM Router delivers value across diverse scenarios for development and operations teams. Whether building new AI-powered applications, modernizing existing systems, or experimenting with models, it provides flexible infrastructure that scales with your needs.',
    'usecases.corporate.title': 'ğŸ¢ Centralized AI Management',
    'usecases.corporate.problem': '<strong>Challenge:</strong> Scattered LLM access across teams without unified governance, security controls, or cost visibility.',
    'usecases.corporate.solution': '<strong>Solution:</strong> LLM Router centralizes all AI traffic through a single gateway, enabling comprehensive data governance, automatic compliance through PII anonymization, and centralized access control before requests reach any provider.',
    'usecases.corporate.feature1': 'Centralized access and policy control',
    'usecases.corporate.feature2': 'Complete audit trail for compliance',
    'usecases.corporate.feature3': 'No rate limiting overhead, optimized throughput',
    'usecases.highavailability.title': 'ğŸ“ˆ High Availability Systems',
    'usecases.highavailability.problem': '<strong>Challenge:</strong> Provider outages cause complete application downtime, disrupting operations.',
    'usecases.highavailability.solution': '<strong>Solution:</strong> Automatic failover between providers with continuous health monitoring and circuit breakers. Achieve high uptime through intelligent load balancing, provider redundancy, and real-time traffic routing with optional in-flight anonymization.',
    'usecases.highavailability.feature1': 'Multi-provider load distribution',
    'usecases.highavailability.feature2': 'Real-time health checks per provider',
    'usecases.highavailability.feature3': 'Automatic failover and circuit breaking',
    'usecases.security.title': 'ğŸ›¡ï¸ Multi-Layer Security',
    'usecases.security.problem': '<strong>Challenge:</strong> Content safety validation and data protection before sending requests to LLM providers.',
    'usecases.security.solution': '<strong>Solution:</strong> LLM Router provides a two-layer security system: guardrail pipelines validate content safety and block prohibited requests, while masking pipelines automatically anonymize sensitive data. Configure sequential processing stages for both guardrails and maskers, ensuring complete protection before requests reach the LLM provider.',
    'usecases.security.feature1': 'Sequential guardrail validation',
    'usecases.security.feature2': 'PII masking with 15+ rule types',
    'usecases.security.feature3': 'Pipeline architecture for composable security layers',
    'usecases.local.title': 'ğŸ–¥ï¸ Self-Hosted Local Models',
    'usecases.local.problem': '<strong>Challenge:</strong> Running models on local infrastructure (vLLM, Ollama, LM Studio) requires managing multiple endpoints and protocols manually.',
    'usecases.local.solution': '<strong>Solution:</strong> LLM Router provides unified management for all your local model deployments. Run models on your own hardware with full control, distribute load across multiple local instances, and optionally add cloud providers as backup. Keep data and processing entirely within your infrastructure.',
    'usecases.local.feature1': 'Full data sovereignty and control',
    'usecases.local.feature2': 'Load balancing across local model instances',
    'usecases.local.feature3': 'Optional cloud fallback for peak loads',
    'usecases.research.title': 'ğŸ”¬ Model Experimentation & A/B Testing',
    'usecases.research.problem': '<strong>Challenge:</strong> Testing different models and providers requires code changes and complex infrastructure setup.',
    'usecases.research.solution': '<strong>Solution:</strong> Experiment with local models (vLLM, Ollama, LM Studio) and cloud providers without any application code changes. Configuration-driven routing enables seamless switching, A/B testing between different models, and cost-optimized strategies that prioritize local deployments.',
    'usecases.research.feature1': 'Compare local vs. cloud model performance',
    'usecases.research.feature2': 'A/B testing any model combination',
    'usecases.research.feature3': 'Cost optimization with local-first routing',
    'usecases.realtime.title': 'ğŸš€ Low-Latency Real-Time Processing',
    'usecases.realtime.problem': '<strong>Challenge:</strong> Processing delays and lack of streaming support impacting user experience in real-time applications.',
    'usecases.realtime.solution': '<strong>Solution:</strong> LLM Router supports Server-Sent Events (SSE) and response streaming with automatic protocol conversion. Optimized for minimal latency with connection pooling, Redis centralization for distributed systems, and non-blocking request handling.',
    'usecases.realtime.feature1': 'SSE streaming <span class="arrow-icon">&#8596;</span> Ollama conversion',
    'usecases.realtime.feature2': 'Connection pooling and pipelining',
    'usecases.realtime.feature3': 'Distributed Redis coordination',
    'usecases.legacy.title': 'âš¡ Legacy System Modernization',
    'usecases.legacy.problem': '<strong>Challenge:</strong> Legacy applications lack modern interfaces for LLM integration, requiring extensive refactoring.',
    'usecases.legacy.solution': '<strong>Solution:</strong> Deploy LLM Router as a unified proxy layer providing consistent REST API access from any applicationâ€”regardless of version, architecture, or programming language. Modernize AI capabilities without refactoring existing codebases.',
    'usecases.legacy.feature1': 'Unified REST API gateway',
    'usecases.legacy.feature2': 'Universal provider compatibility',
    'usecases.legacy.feature3': 'Zero-touch legacy integration',

    // Open Source
    'opensource.title': 'Open Source & Community-Driven',
    'opensource.subtitle': 'Transparent, extensible, production-ready',
    'opensource.license': 'License:',
    'opensource.description': 'LLM Router is <strong>fully open source</strong> under the permissive <strong>Apache 2.0</strong> license, providing complete freedom for <strong>commercial use</strong>, <strong>modification</strong>, and <strong>distribution</strong>. Built on community collaboration with transparent architecture and auditable code, you can customize it for your specific needs, contribute improvements, and benefit from ongoing community enhancementsâ€”all without vendor lock-in or licensing constraints.',
    'opensource.cta.star': 'â­ Star on GitHub',
    'opensource.cta.issue': 'ğŸ’¡ Report Issue',
    'opensource.cta.contribute': 'ğŸ”§ Contribute',

    // Contact
    'contact.title': 'Contact',
    'contact.subtitle': 'Have questions? Write to us, and we\'ll respond in a flash!',

    // Footer
    'footer.project': 'Project',
    'footer.project.repo': 'GitHub Repository',
    'footer.project.docs': 'Documentation',
    'footer.project.changelog': 'Changelog',
    'footer.project.license': 'License',
    'footer.components': 'Components',
    'footer.community': 'Community',
    'footer.community.issues': 'Issue Tracker',
    'footer.community.pulls': 'Pull Requests',
    'footer.community.discussions': 'Discussions',
    'footer.community.radlab': 'RadLab Dev Group',
    'footer.resources': 'Resources',
    'footer.resources.quickstart': 'Quick Start',
    'footer.resources.install': 'Installation',
    'footer.resources.docker': 'Docker Images',
    'footer.resources.mlutils': 'ML Utils (dependency)',
    'footer.copyright': 'Â© 2025 LLM-Router | Open Source Project | Licensed under Apache 2.0',

    // Back to top
    'backToTop.aria': 'Back to top',
  }
};
